{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL0321EN-4-1-Comparing-Models-py-v1.0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im7Jke0bMZnj"
      },
      "source": [
        "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n",
        "\n",
        "<h1 align=center><font size = 5>Peer Review Final Assignment</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWFf8FI3MZns"
      },
      "source": [
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHSgzR2RMZnw"
      },
      "source": [
        "In this lab, you will build an image classifier using the VGG16 pre-trained model, and you will evaluate it and compare its performance to the model we built in the last module using the ResNet50 pre-trained model. Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "883qEQEkMZn1"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>    \n",
        "\n",
        "1. <a href=\"#item41\">Download Data \n",
        "2. <a href=\"#item42\">Part 1</a>\n",
        "3. <a href=\"#item43\">Part 2</a>  \n",
        "4. <a href=\"#item44\">Part 3</a>  \n",
        "\n",
        "</font>\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IFWp09VMZn4"
      },
      "source": [
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w62sfYonMZn8"
      },
      "source": [
        "<a id=\"item41\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uebYvG3vMZoA"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tPo82ACMZoC"
      },
      "source": [
        "Use the <code>wget</code> command to download the data for this assignment from here: https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNgRPjgBMZoF"
      },
      "source": [
        "Use the following cells to download the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKwow1wQcjkg",
        "outputId": "339edeeb-fa92-4f28-cb49-e55e4f594d91"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 10173735791472240765, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11154422528\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 5318758604266751051\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw1tv7t_MZoI",
        "outputId": "d89fdf6d-35e1-40c7-aaba-4d278498b923"
      },
      "source": [
        "!rm concrete_data_week4.zip concrete_data_week4.zip.1\n",
        "\n",
        "!rm -r concrete_data_week4\n",
        "\n",
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'concrete_data_week4.zip': No such file or directory\n",
            "rm: cannot remove 'concrete_data_week4.zip.1': No such file or directory\n",
            "rm: cannot remove 'concrete_data_week4': No such file or directory\n",
            "--2021-04-07 09:28:23--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 261483817 (249M) [application/zip]\n",
            "Saving to: ‘concrete_data_week4.zip’\n",
            "\n",
            "concrete_data_week4 100%[===================>] 249.37M  25.2MB/s    in 9.3s    \n",
            "\n",
            "2021-04-07 09:28:33 (26.9 MB/s) - ‘concrete_data_week4.zip’ saved [261483817/261483817]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgW8AnsFMZoL",
        "outputId": "0435ddfa-a53f-47c8-f09e-2e25053104e4"
      },
      "source": [
        "!unzip concrete_data_week4.zip > unzip_output.out\n",
        "\n",
        "print('unzipped')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzipped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BHIfK3dMZoR"
      },
      "source": [
        "After you unzip the data, you fill find the data has already been divided into a train, validation, and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhJaUmuYMZoU"
      },
      "source": [
        "  # I had some problems in the previous lab in order to train the model because the lab kept crashing therefore I am training both models here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui6wTYV-MZoV"
      },
      "source": [
        "<a id=\"item42\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F27IqvSNMZoX"
      },
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehpLYMD4MZoe"
      },
      "source": [
        "In this part, you will design a classifier using the VGG16 pre-trained model. Just like the ResNet50 model, you can import the model <code>VGG16</code> from <code>keras.applications</code>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l32vP1mXMZon"
      },
      "source": [
        "You will essentially build your classifier as follows:\n",
        "1. Import libraries, modules, and packages you will need. Make sure to import the *preprocess_input* function from <code>keras.applications.vgg16</code>.\n",
        "2. Use a batch size of 100 images for both training and validation.\n",
        "3. Construct an ImageDataGenerator for the training set and another one for the validation set. VGG16 was originally trained on 224 × 224 images, so make sure to address that when defining the ImageDataGenerator instances.\n",
        "4. Create a sequential model using Keras. Add VGG16 model to it and dense layer.\n",
        "5. Compile the mode using the adam optimizer and the categorical_crossentropy loss function.\n",
        "6. Fit the model on the augmented data using the ImageDataGenerators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtFtanTwMZo0"
      },
      "source": [
        "Use the following cells to create your classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8oPIrXhMZo4"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "from keras.applications import VGG16 as VVG16 #had a typo\n",
        "from keras.applications import ResNet50\n",
        "\n",
        "from keras.applications.vgg16 import preprocess_input as vvg16_preprocess_input\n",
        "from keras.applications.resnet50 import preprocess_input as resnet50_preprocess_input"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97mMsOVfMZo7"
      },
      "source": [
        "num_classes = 2\n",
        "\n",
        "image_resize = 224\n",
        "\n",
        "batch_size_training = 100\n",
        "batch_size_validation = 100"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h2ELXxWMZo7",
        "scrolled": true
      },
      "source": [
        "## training vvg16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oH59rEDMZo9"
      },
      "source": [
        "vvg16_data_generator = ImageDataGenerator(\n",
        "    preprocessing_function=vvg16_preprocess_input,\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOw_SQhUMZo_",
        "outputId": "da302feb-967c-4391-e561-ff22245008e4"
      },
      "source": [
        "vvg16_train_generator = vvg16_data_generator.flow_from_directory(\n",
        "    'concrete_data_week4/train',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_training,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 30001 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCe6BBsiMZpB",
        "outputId": "5adb1891-1aa6-491f-fab6-972d373b9d13"
      },
      "source": [
        "vvg16_validation_generator = vvg16_data_generator.flow_from_directory(\n",
        "    'concrete_data_week4/valid',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_training,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9501 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ei7XErDMZpE",
        "outputId": "36b2cbca-e50d-4c86-bfa5-ff1a67f2bdb1"
      },
      "source": [
        "vvg16_model = Sequential()\n",
        "\n",
        "vvg16_model.add(VVG16(\n",
        "    include_top=False,\n",
        "    pooling='avg',\n",
        "    weights='imagenet',\n",
        "    ))\n",
        "\n",
        "vvg16_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "vvg16_model.layers[0].trainable = False\n",
        "\n",
        "vvg16_model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 512)               14714688  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 14,715,714\n",
            "Trainable params: 1,026\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhIaElwCMZpG",
        "outputId": "db1bcad5-2b10-4b2c-a0ef-207ba0103da3"
      },
      "source": [
        "vvg16_model.layers"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.engine.functional.Functional at 0x7fa00fdb6f90>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fa00fddcad0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1S3YwsnMZpJ",
        "outputId": "a56f52c7-a781-4c4d-b439-13515dd0c2e6"
      },
      "source": [
        "vvg16_model.layers[0].layers"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fa0931cc050>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa0931cd750>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa00fe89c50>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fa00fe0a490>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa00fe61910>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa00fe10d10>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fa00fe18e10>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa057d50450>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa057d45210>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa057df9710>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fa057e34dd0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa05e531150>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa05e5a9a10>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa00fe1e090>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fa00fe18ad0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa00fe102d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa00fe10590>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa00fe22b10>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fa00fdb1a10>,\n",
              " <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x7fa00fdb6f10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8NQ_pQ0MZpK"
      },
      "source": [
        "vvg16_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLwbivbRMZpN"
      },
      "source": [
        "vvg16_steps_per_epoch_training = len(vvg16_train_generator)\n",
        "vvg16_steps_per_epoch_validation = len(vvg16_validation_generator)\n",
        "num_epochs = 2"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPrHOSu-MZpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528bdc66-eccc-463d-a7d0-d8158c2b221c"
      },
      "source": [
        "vvg16_fit_history = vvg16_model.fit_generator(\n",
        "    vvg16_train_generator,\n",
        "    steps_per_epoch=vvg16_steps_per_epoch_training,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=vvg16_validation_generator,\n",
        "    validation_steps=vvg16_steps_per_epoch_validation,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "301/301 [==============================] - 342s 958ms/step - loss: 0.4285 - accuracy: 0.8620 - val_loss: 0.0377 - val_accuracy: 0.9924\n",
            "Epoch 2/2\n",
            "301/301 [==============================] - 287s 954ms/step - loss: 0.0334 - accuracy: 0.9933 - val_loss: 0.0217 - val_accuracy: 0.9952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwVH5ndfMZpP"
      },
      "source": [
        "vvg16_model.save('classifier_vvg16_model.h5')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uacqWh_YMZpQ"
      },
      "source": [
        "## training resnet 50\n",
        "\n",
        "because the previous lab kept crushing, so I have not been able to train it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39Tmk07GMZpS"
      },
      "source": [
        "resnet50_data_generator = ImageDataGenerator(\n",
        "    preprocessing_function=resnet50_preprocess_input,\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0rqIA1qMZpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032966c6-e58d-45c7-f46a-4dc6c5d662dc"
      },
      "source": [
        "resnet50_train_generator = resnet50_data_generator.flow_from_directory(\n",
        "    'concrete_data_week4/train',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_training,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 30001 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyugDVBGMZpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961090a3-0691-4cb2-adbc-02ee47295638"
      },
      "source": [
        "resnet50_validation_generator = resnet50_data_generator.flow_from_directory(\n",
        "    'concrete_data_week4/valid',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_training,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9501 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzIi49tXMZpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412f71ff-b93c-4f19-812d-ac816fdcdadc"
      },
      "source": [
        "resnet50_model = Sequential()\n",
        "\n",
        "resnet50_model.add(ResNet50(\n",
        "    include_top=False,\n",
        "    pooling='avg',\n",
        "    weights='imagenet',\n",
        "    ))\n",
        "\n",
        "resnet50_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "resnet50_model.layers[0].trainable = False\n",
        "\n",
        "resnet50_model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 4098      \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHfmutDNMZpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eaf9b38-cb4e-46b2-e650-617ebf4ddab7"
      },
      "source": [
        "resnet50_model.layers"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.engine.functional.Functional at 0x7f9fa7179e90>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fa00f120e50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoidNSWxMZpZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd948fad-1835-4da1-afc4-96455a71de64"
      },
      "source": [
        "resnet50_model.layers[0].layers"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fa00f0bd290>,\n",
              " <tensorflow.python.keras.layers.convolutional.ZeroPadding2D at 0x7fa00f0bded0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa00f97a250>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fa0604bd690>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fa05e5a9190>,\n",
              " <tensorflow.python.keras.layers.convolutional.ZeroPadding2D at 0x7fa00f123410>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fa00f1208d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa00fdb6510>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa73f1690>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7462390>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7462a10>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7403e50>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa73fe450>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa00fdb6410>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa73fef10>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7462dd0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7414a50>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa73fc7d0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa73fe550>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa73fe790>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa753c6d0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa753c5d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa741d410>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa742a690>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa742a390>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa742a650>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa73b0890>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa7411e10>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa73bb310>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7421050>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa73c85d0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa73c89d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa73c8590>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa73b5550>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa73cce50>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa73cc310>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa73dde10>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa73b5cd0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7379190>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7373210>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7391650>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa73916d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa5e2b2d0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa739a8d0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7385dd0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7379210>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa733fb90>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa737fad0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7347b10>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa7333510>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7350dd0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7356050>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa735b5d0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa734b590>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7366a90>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa72ecf50>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa72f3390>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa72f36d0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa72f59d0>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa7333710>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa739a0d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7366f90>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7366fd0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa735b510>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa73ddf50>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa736f390>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7395550>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7395e10>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa73c8910>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa73a5450>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa73c2cd0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7417fd0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa746a050>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa742ab50>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa741d910>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa73033d0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7392890>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7392310>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7303f90>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa73064d0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7318150>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa73506d0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa72aedd0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa73228d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa73226d0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa72b6c90>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa72ccb10>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7318610>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa73225d0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa731be90>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa72d0a50>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa7318910>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa72e52d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa72c8c10>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa72d8bd0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa727c850>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa72eb090>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7275890>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa728c610>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa728c250>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa727c710>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa72872d0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa72d0a90>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa72e5610>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa72e50d0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7292d50>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7318450>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7318d50>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa72bf810>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7313090>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7414850>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa7462850>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa737f550>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7330c50>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa73473d0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7350a90>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7347d90>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7350590>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa73dd890>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa73d6550>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa72a6750>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa72ebc10>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa722c450>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7235c90>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa723c650>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7235350>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa72356d0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa723cd90>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7248290>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7258910>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7252d50>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa7244990>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa71ef4d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa71ef550>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa71f5f90>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa726a750>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa71ef090>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7200e10>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7211950>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7211090>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7200fd0>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa71acf90>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa71acd90>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa71ef190>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa71efa10>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa71b0090>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7258d90>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7244c50>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa72359d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa71ac290>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa71f5f50>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa721d050>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7313cd0>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa73c2310>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa72b6ad0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa72b2ed0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7292290>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa71b8890>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa7264fd0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa7318350>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa7250750>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa73cc390>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa71cc990>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa71be990>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa71dc790>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa71dc650>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa71e3890>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa71d8210>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa71dcf90>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa71e3910>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa716d4d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f9fa717ea50>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f9fa71e3c10>,\n",
              " <tensorflow.python.keras.layers.merge.Add at 0x7f9fa718b490>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7f9fa71a2c10>,\n",
              " <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x7f9fa719be90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APDc8KGYMZpZ"
      },
      "source": [
        "resnet50_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFI7eiQ-MZpb"
      },
      "source": [
        "resnet50_steps_per_epoch_training = len(resnet50_train_generator)\n",
        "resnet50_steps_per_epoch_validation = len(resnet50_validation_generator)\n",
        "#num_epochs = 2"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF8ZlaanMZpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a461302-1b5f-4277-f209-d25a6f3fc485"
      },
      "source": [
        "resnet50_fit_history = resnet50_model.fit_generator(\n",
        "    resnet50_train_generator,\n",
        "    steps_per_epoch=resnet50_steps_per_epoch_training,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=resnet50_validation_generator,\n",
        "    validation_steps=resnet50_steps_per_epoch_validation,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "301/301 [==============================] - 258s 837ms/step - loss: 0.0574 - accuracy: 0.9811 - val_loss: 0.0057 - val_accuracy: 0.9988\n",
            "Epoch 2/2\n",
            "301/301 [==============================] - 250s 831ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0047 - val_accuracy: 0.9985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhumHZbMMZpd"
      },
      "source": [
        "vvg16_model.save('classifier_resnet50_model.h5')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzRJOMP3MZpf"
      },
      "source": [
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1Yyw4YTMZpf"
      },
      "source": [
        "<a id=\"item43\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyVohDjOMZpg"
      },
      "source": [
        "## Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-1KJHcKMZph"
      },
      "source": [
        "In this part, you will evaluate your deep learning models on a test data. For this part, you will need to do the following:\n",
        "\n",
        "1. Load your saved model that was built using the ResNet50 model. \n",
        "2. Construct an ImageDataGenerator for the test set. For this ImageDataGenerator instance, you only need to pass the directory of the test images, target size, and the **shuffle** parameter and set it to False.\n",
        "3. Use the **evaluate_generator** method to evaluate your models on the test data, by passing the above ImageDataGenerator as an argument. You can learn more about **evaluate_generator** [here](https://keras.io/models/sequential/).\n",
        "4. Print the performance of the classifier using the VGG16 pre-trained model.\n",
        "5. Print the performance of the classifier using the ResNet pre-trained model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIt_IjucMZpi"
      },
      "source": [
        "Use the following cells to evaluate your models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS15aQK-MZpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c16b88-0444-4f38-f604-1926ba0018fc"
      },
      "source": [
        "resnet50_test_generator = resnet50_data_generator.flow_from_directory(\n",
        "    'concrete_data_week4/test',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    shuffle=False)\n",
        "\n",
        "vvg16_test_generator = vvg16_data_generator.flow_from_directory(\n",
        "    'concrete_data_week4/test',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    shuffle=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 500 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlSroCJiMZpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8736c74f-02ad-43de-b31d-3127bfce0247"
      },
      "source": [
        "resnet_result = resnet50_model.evaluate_generator(resnet50_test_generator)\n",
        "\n",
        "print('resnet ', resnet_result)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "resnet  [0.002283327979966998, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV-0Bk6SMZpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513fd62e-1c41-4914-ff1c-a2f43415e459"
      },
      "source": [
        "vvg16_result = vvg16_model.evaluate_generator(vvg16_test_generator)\n",
        "\n",
        "print('vvg16 ', vvg16_result)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "vvg16  [0.019957879558205605, 0.9959999918937683]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LeIq6vUMZpq"
      },
      "source": [
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyS5-ozaMZpr"
      },
      "source": [
        "<a id=\"item44\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmOmNc1-MZpu"
      },
      "source": [
        "## Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Osg_wSMZpv"
      },
      "source": [
        "In this model, you will predict whether the images in the test data are images of cracked concrete or not. You will do the following:\n",
        "\n",
        "1. Use the **predict_generator** method to predict the class of the images in the test data, by passing the test data ImageDataGenerator instance defined in the previous part as an argument. You can learn more about the **predict_generator** method [here](https://keras.io/models/sequential/).\n",
        "2. Report the class predictions of the first five images in the test set. You should print something list this:\n",
        "\n",
        "<center>\n",
        "    <ul style=\"list-style-type:none\">\n",
        "        <li>Positive</li>  \n",
        "        <li>Negative</li> \n",
        "        <li>Positive</li>\n",
        "        <li>Positive</li>\n",
        "        <li>Negative</li>\n",
        "    </ul>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L9nZN1WMZpx"
      },
      "source": [
        "Use the following cells to make your predictions.\n",
        "\n",
        "# Had a small annoing problem with the notebook therefore I reload the models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2OPlC8Ul9nt"
      },
      "source": [
        "import keras"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEkrCViXl-fP",
        "outputId": "1bf866e6-7001-4ca9-fff9-40795ec59fc7"
      },
      "source": [
        "resnet50_model = keras.models.load_model('classifier_resnet50_model.h5')\n",
        "\n",
        "resnet_result = resnet50_model.predict_generator(resnet50_test_generator)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVTi0p5CpCIm",
        "outputId": "78132ff9-5f86-4f3a-e4f3-4a187f98b724"
      },
      "source": [
        "def print_pos_neg(result):\n",
        "\n",
        "  if result[0] > result[1]:\n",
        "\n",
        "    print('Negative')\n",
        "\n",
        "  else:\n",
        "\n",
        "    print('Positive')\n",
        "\n",
        "print('resnet:')\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  print_pos_neg(resnet_result[i])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "resnet:\n",
            "Negative\n",
            "Negative\n",
            "Negative\n",
            "Negative\n",
            "Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fsGLSkTMZp3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38befbfe-a32e-4583-883c-0a5850c9f03f"
      },
      "source": [
        "vvg16_model = keras.models.load_model('classifier_vvg16_model.h5')\n",
        "\n",
        "vvg16_result = vvg16_model.predict_generator(vvg16_test_generator)\n",
        "\n",
        "print('vvg16:')\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  print_pos_neg(vvg16_result[i])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "vvg16:\n",
            "Negative\n",
            "Negative\n",
            "Negative\n",
            "Negative\n",
            "Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXGsHvqdMZp5"
      },
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7DugsFrMZp6"
      },
      "source": [
        "### Thank you for completing this lab!\n",
        "\n",
        "This notebook was created by Alex Aklson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIOzcXb-MZp7"
      },
      "source": [
        "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week4_LAB1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muUI7LkVMZp8"
      },
      "source": [
        "<hr>\n",
        "\n",
        "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
      ]
    }
  ]
}